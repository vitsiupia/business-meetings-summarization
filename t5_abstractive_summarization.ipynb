{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vitsiupia/projektPython/blob/main/t5_abstractive_summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Business Meeting Summary Generation Using T5"
      ],
      "metadata": {
        "id": "7dXbsjKS7EtM"
      },
      "id": "7dXbsjKS7EtM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1b46033-6297-42d2-acba-c97b8dbea6b3",
      "metadata": {
        "id": "d1b46033-6297-42d2-acba-c97b8dbea6b3"
      },
      "outputs": [],
      "source": [
        "!pip install keras_nlp==0.3.0\n",
        "!pip install huggingface-hub\n",
        "!pip install datasets transformers rouge-score nltk\n",
        "!apt install git-lfs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "vhShr7vLeDk5"
      },
      "id": "vhShr7vLeDk5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the necessary libraries\n",
        "import os\n",
        "import logging\n",
        "import nltk\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "\n",
        "# Only log error messages\n",
        "tf.get_logger().setLevel(logging.ERROR)\n",
        "\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL']"
      ],
      "metadata": {
        "id": "24hq9GePlpGx"
      },
      "id": "24hq9GePlpGx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4d26ae33",
      "metadata": {
        "id": "4d26ae33"
      },
      "outputs": [],
      "source": [
        "gpu_devices = tf.config.experimental.list_physical_devices('GPU')\n",
        "for device in gpu_devices:\n",
        "    tf.config.experimental.set_memory_growth(device, True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the Data"
      ],
      "metadata": {
        "id": "-le4kXJhgvVJ"
      },
      "id": "-le4kXJhgvVJ"
    },
    {
      "cell_type": "markdown",
      "id": "04551bb4-c2fb-4249-8b0f-d911737594f1",
      "metadata": {
        "id": "04551bb4-c2fb-4249-8b0f-d911737594f1"
      },
      "source": [
        "Przygotujmy nasz korpus, żeby wyglądał dokładnie jak datasets pobrany z Huggingface, czyli miał kolumny jak: 'document' 'summary' 'id'. Zróbmy to w pandas dataframe...\n",
        "\n",
        "... A później przerobimy na HF dataset takim kodem:\n",
        "\n",
        "```\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "df = pd.DataFrame({\"a\": [1, 2, 3]})\n",
        "dataset = Dataset.from_pandas(df)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4d0d67af-7569-4aa2-98f7-148ec2b01bc6",
      "metadata": {
        "scrolled": true,
        "id": "4d0d67af-7569-4aa2-98f7-148ec2b01bc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19a7b2d0-ed47-4c4b-c04a-2c237fee61ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-01 16:12:02--  https://github.com/vitsiupia/projektPython/raw/main/meetings_split.zip\n",
            "Resolving github.com (github.com)... 140.82.112.4\n",
            "Connecting to github.com (github.com)|140.82.112.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/vitsiupia/projektPython/main/meetings_split.zip [following]\n",
            "--2023-06-01 16:12:02--  https://raw.githubusercontent.com/vitsiupia/projektPython/main/meetings_split.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1034366 (1010K) [application/zip]\n",
            "Saving to: ‘meetings_split.zip.1’\n",
            "\n",
            "meetings_split.zip. 100%[===================>]   1010K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-06-01 16:12:03 (18.1 MB/s) - ‘meetings_split.zip.1’ saved [1034366/1034366]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Download our dataset with splits.\n",
        "!wget https://github.com/vitsiupia/projektPython/raw/main/meetings_split.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "864a93b1-70ba-46e0-8261-c76f7d7e7d74",
      "metadata": {
        "id": "864a93b1-70ba-46e0-8261-c76f7d7e7d74"
      },
      "outputs": [],
      "source": [
        "# Unpack the dataset.\n",
        "with zipfile.ZipFile('meetings_split.zip', 'r') as zip:\n",
        "  zip.extractall()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = []\n",
        "meetings_folder = \"meetings_split\"\n",
        "transcripts_folder, summaries_folder = \"transcripts\", \"summaries\"\n",
        "\n",
        "# Przeszukiwanie folderów train/, val/, test/\n",
        "for split in ['train','val', 'test']:\n",
        "    transcripts_dir = os.path.join(meetings_folder, split, transcripts_folder)\n",
        "    \n",
        "    # Przechodzenie przez każdy transkrypt w folderzeю\n",
        "    for transcript_name in os.listdir(transcripts_dir):\n",
        "          code = transcript_name.split(\".\")[0]\n",
        "            \n",
        "          # Read the text inside the transcript.\n",
        "          with open(os.path.join(transcripts_dir, transcript_name), \"r\") as file:\n",
        "              transcript = file.read().strip()\n",
        "\n",
        "          # Now read the text in the respective summary for train, val folders.\n",
        "          if split != 'test':\n",
        "              summary_name = code + \".abssumm.txt\"\n",
        "              summaries_dir = os.path.join(meetings_folder, split, summaries_folder)\n",
        "              with open(os.path.join(summaries_dir,summary_name), \"r\") as file:\n",
        "                  summary = file.read().strip()\n",
        "          else:\n",
        "              summary = ''\n",
        "            \n",
        "          data.append({\n",
        "              \"transcript\": transcript,\n",
        "               \"summary\": summary,\n",
        "               \"code\": code,\n",
        "               \"split\" : split,\n",
        "          })\n",
        "          # Usunięcie zmiennych transcript_text i summary_text\n",
        "          del transcript, summary\n",
        "\n",
        "# Tworzenie DataFrame z zebranych danych\n",
        "ami_df = pd.DataFrame(data)"
      ],
      "metadata": {
        "id": "hAGQkWE_xo-N"
      },
      "id": "hAGQkWE_xo-N",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ami_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "sSHIXIhp0QDi",
        "outputId": "e3e4d800-2286-47ee-d3db-6019ce8d620d"
      },
      "id": "sSHIXIhp0QDi",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            transcript  \\\n",
              "0    nick industrial designer tool training yes you...   \n",
              "1    maybe maybe maybe maybe bra thats thats mean g...   \n",
              "2    help screens black thats fine done right secon...   \n",
              "3    go slides number three number two sorry final ...   \n",
              "4    making slide open underneath fold open dont kn...   \n",
              "..                                                 ...   \n",
              "166  make point well make point make status shot de...   \n",
              "167  assume well youve sort information well got gd...   \n",
              "168  summary basing certain thresholds special that...   \n",
              "169  wonder much meetings talking stuff meetings lo...   \n",
              "170  could ju quickly rewind wait wait going fast a...   \n",
              "\n",
              "                                               summary     code  split  \n",
              "0    The group introduced themselves and their role...  ES2014a  train  \n",
              "1    The User Interface Designer and the Industrial...  IS1002d  train  \n",
              "2    The Industrial Designer gave his presentation ...  ES2004b  train  \n",
              "3    The project manager goes through the minutes o...  IS1004d  train  \n",
              "4    The project manager presented the agenda and t...  TS3004d  train  \n",
              "..                                                 ...      ...    ...  \n",
              "166                                                      IN1001   test  \n",
              "167                                                     EN2009d   test  \n",
              "168                                                     EN2001d   test  \n",
              "169                                                     EN2002a   test  \n",
              "170                                                      IN1016   test  \n",
              "\n",
              "[171 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-281be253-848d-459c-a3ce-8bf620ac7cde\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>transcript</th>\n",
              "      <th>summary</th>\n",
              "      <th>code</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>nick industrial designer tool training yes you...</td>\n",
              "      <td>The group introduced themselves and their role...</td>\n",
              "      <td>ES2014a</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>maybe maybe maybe maybe bra thats thats mean g...</td>\n",
              "      <td>The User Interface Designer and the Industrial...</td>\n",
              "      <td>IS1002d</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>help screens black thats fine done right secon...</td>\n",
              "      <td>The Industrial Designer gave his presentation ...</td>\n",
              "      <td>ES2004b</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>go slides number three number two sorry final ...</td>\n",
              "      <td>The project manager goes through the minutes o...</td>\n",
              "      <td>IS1004d</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>making slide open underneath fold open dont kn...</td>\n",
              "      <td>The project manager presented the agenda and t...</td>\n",
              "      <td>TS3004d</td>\n",
              "      <td>train</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>166</th>\n",
              "      <td>make point well make point make status shot de...</td>\n",
              "      <td></td>\n",
              "      <td>IN1001</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>167</th>\n",
              "      <td>assume well youve sort information well got gd...</td>\n",
              "      <td></td>\n",
              "      <td>EN2009d</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>168</th>\n",
              "      <td>summary basing certain thresholds special that...</td>\n",
              "      <td></td>\n",
              "      <td>EN2001d</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>169</th>\n",
              "      <td>wonder much meetings talking stuff meetings lo...</td>\n",
              "      <td></td>\n",
              "      <td>EN2002a</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>170</th>\n",
              "      <td>could ju quickly rewind wait wait going fast a...</td>\n",
              "      <td></td>\n",
              "      <td>IN1016</td>\n",
              "      <td>test</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>171 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-281be253-848d-459c-a3ce-8bf620ac7cde')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-281be253-848d-459c-a3ce-8bf620ac7cde button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-281be253-848d-459c-a3ce-8bf620ac7cde');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ami_df.loc[ami_df.code.duplicated()].sort_values(by='code')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49
        },
        "id": "9DJkpL_TqO3H",
        "outputId": "91bd2f5b-68a0-4904-e108-534b19e59c5f"
      },
      "id": "9DJkpL_TqO3H",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [transcript, summary, code, split]\n",
              "Index: []"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-01d413fe-ad53-4099-87f9-e983407226e8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>transcript</th>\n",
              "      <th>summary</th>\n",
              "      <th>code</th>\n",
              "      <th>split</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-01d413fe-ad53-4099-87f9-e983407226e8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-01d413fe-ad53-4099-87f9-e983407226e8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-01d413fe-ad53-4099-87f9-e983407226e8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ef3fc2c9-c8ff-4840-887b-2a35c8625a16",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef3fc2c9-c8ff-4840-887b-2a35c8625a16",
        "outputId": "e7c269f6-eaaf-4c3c-fb18-cf404c4c5e6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ilość plików w folderze 'meetings_split/train/transcripts': 120\n",
            "Ilość plików w folderze 'meetings_split/train/summaries': 120\n",
            "Ilość plików w folderze 'meetings_split/val/transcripts': 22\n",
            "Ilość plików w folderze 'meetings_split/val/summaries': 22\n",
            "Ilość plików w folderze 'meetings_split/test/transcripts': 29\n"
          ]
        }
      ],
      "source": [
        "for folder_path in [\"meetings_split/train/transcripts\", \"meetings_split/train/summaries\", \n",
        "                    \"meetings_split/val/transcripts\", \"meetings_split/val/summaries\", \"meetings_split/test/transcripts\"]:\n",
        "  file_count = len(os.listdir(folder_path))\n",
        "  print(f\"Ilość plików w folderze '{folder_path}': {file_count}\") "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use the [🤗 Datasets](https://github.com/huggingface/datasets) library to prepare the data and get the metric we need to use for evaluation (to compare our model to the benchmark)."
      ],
      "metadata": {
        "id": "-MOb9hvkhCAJ"
      },
      "id": "-MOb9hvkhCAJ"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "65569d1a-af62-4d71-9c25-5f2395ad54f4",
      "metadata": {
        "id": "65569d1a-af62-4d71-9c25-5f2395ad54f4"
      },
      "outputs": [],
      "source": [
        "# # Define certain variables\n",
        "\n",
        "# # Z wcześniejszego notatnika:\n",
        "# # ...Najdłuższy transkrypt ma 3870 słów.\n",
        "# # ...Najdłuższe podsumowanie ma 530 słów.\n",
        "# # ...Najkrótsze podsumowanie ma 41 słów.\n",
        "\n",
        "# MAX_INPUT_LENGTH = 1024\n",
        "# MIN_TARGET_LENGTH = 10\n",
        "# MAX_TARGET_LENGTH = 128\n",
        "# BATCH_SIZE = 50  # Batch-size for training our model\n",
        "# LEARNING_RATE = 2e-3  # Learning-rate for training our model\n",
        "# MAX_EPOCHS = 1  # Maximum number of epochs we will train the model for\n",
        "\n",
        "# # This notebook is built on the t5-small checkpoint from the Hugging Face Model Hub\n",
        "# MODEL_CHECKPOINT = \"t5-small\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "6f0b42a8-1eb6-44a5-ac7b-e5284f4cd140",
      "metadata": {
        "id": "6f0b42a8-1eb6-44a5-ac7b-e5284f4cd140"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "\n",
        "# Split the DataFrame into train, validation and test DataFrames based on the 'split' column\n",
        "train_df = ami_df[ami_df['split'] == 'train']\n",
        "val_df = ami_df[ami_df['split'] == 'val']\n",
        "test_df = ami_df[ami_df['split'] == 'test']\n",
        "\n",
        "# Create datasets.Dataset objects from the train and validation DataFrames\n",
        "train_dataset = datasets.Dataset.from_pandas(train_df.drop('split', axis=1))\n",
        "val_dataset = datasets.Dataset.from_pandas(val_df.drop('split', axis=1))\n",
        "test_dataset = datasets.Dataset.from_pandas(test_df.drop('split', axis=1))\n",
        "\n",
        "# Create a DatasetDict containing train, val, test datasets\n",
        "raw_datasets = datasets.DatasetDict({\n",
        "    'train': train_dataset,\n",
        "    'val': val_dataset,\n",
        "    'test': test_dataset\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `dataset` object itself is [`DatasetDict`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasetdict), which contains one key for the training, validation and test set:"
      ],
      "metadata": {
        "id": "TjlJZGuphPTv"
      },
      "id": "TjlJZGuphPTv"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "408c6494-1348-4d7f-bdb0-dc818319e4c8",
      "metadata": {
        "id": "408c6494-1348-4d7f-bdb0-dc818319e4c8",
        "outputId": "b7d0456e-3f3d-47d0-ec20-4c1def00fdc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['transcript', 'summary', 'code', '__index_level_0__'],\n",
              "        num_rows: 120\n",
              "    })\n",
              "    val: Dataset({\n",
              "        features: ['transcript', 'summary', 'code', '__index_level_0__'],\n",
              "        num_rows: 22\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['transcript', 'summary', 'code', '__index_level_0__'],\n",
              "        num_rows: 29\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "raw_datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_datasets[\"train\"][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6gp2EKce4c5",
        "outputId": "82735aed-3324-4c0d-a781-a89cab0d3fdd"
      },
      "id": "o6gp2EKce4c5",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'transcript': 'nick industrial designer tool training yes youve lost microphone ill ill shall see get across without tangling everything theres one didnt think pens ill try red pen gonna go bear able draw well ill bash ooh ooh lost think ive knocked microphone well g well go small small bear animal looks nothing bear dunno maybe theres many cartoon characters made bear jungle book characters stuff great yes remote controls buttons little small theyre quite hard press maybe make something easy press buttons main function yes sort easy use buttons accessible easy use see yes sorry go go yes maybe could better instructions remote design remote control sort instructions would come slide four yes yes think said start television remote control maybe stick unless get told otherwise thats great right start first meeting right agenda first meeting twenty five minutes meeting get acquainted everyone want say seem sensible alastair project leader tool training project plan anyone thoughts tool training required neither see shouldnt really right project team basically come new r remote control device starting base original existence period time idea make new remote control device user friendly previous one trendier therefore get bigger market share bigger audience method split see functional design conceptional design detailed design phases well basically handing designers device meetings course day come better better inst implement therefore successful conclusion day youll various designs throughout day meet end weve got tool training try whiteboard right everyones supposedly draw favourite animal white board guess make sure whiteboard works dont know wishes go first wish go f first bash whatever ah youll move microphone camera take would would guess technical problems mean designers meant come sort things next dont see theres need theres plenty space mean whatever exactly weve time prepare side weve stuck bits pieces pockets three pens underneath get marks artistic impression youre face right well ive actually many pets time cause honest keen anyway worry daughters got moment theyve got fish hopefully wont prove difficult draw see artist artistic work useless well anyway one best things fish dont really take much looking animals youre going away holiday whatever youve gotta spend money get friend whatever look whereas got fish gotta put food dripper feed feeds couple weeks youre away change water every couple months buy plants fact keep dying fish reasonable pets theyre low maintenance right still us right work done project twenty five euros expected selling price information come marketing manager looking sell internationally europe looking production costs limited twelve half euro per unit therefore making profit margin well actually profit margin obviously youre gonna overheads various costs take give profit margin per unit depending overhead costs determine many units looking sell projecting sell point time experience remote control first ideas new remote guess looking discussion point time help folks design new model thoughts basically looking looking device robust therefore wont get damaged easily looking device things said easy use use see w basically ill get back seems sensible cause rightly said theres nothing annoying three four devices littered room device remotes sense r mutually exclusive cant th one device buttons cause want want simplicity well want idiot able use whilst time want rightly said one remote probably mutually exclusive options could argue experience using devices similar devices people get used using remotes therefore theyre handling therefore make complicated time goes better instructions well weve got five minutes end meeting start winding next meeting thirty minutes right weve got id come wheres go thats looking right weve got function happened right right sorry weve got working design id uid technical functions design marketing user requirement specification specific instructions sent person personal coach clear objectives looking meet next thirty minutes guess ill try write minutes meeting give next meeting television remote control thats true cause course day might make decisions based information meetings would change going point time think youre right shall make tv depart stay break ill minutes well see half hour right robin marketing manager dunno microphones dont know whole menagerie right hello gonna go dog gonna draw one badly well looks going dachshund something right theres dog dogs theyre loyal theyre always happy whenever whenever youre feeling sort bit bit tired theyre always coming theyre always quite excited always lot fun dog theyre also good exercise well sorta get sorta never get tired theyre tired theyre quite cute well thats dogs great ive one things found market research people often get confused number buttons well cause theres quite often lots lots sometimes sort remote controls defeat purpose youre sat chair remote somewhere else room whereas past youd get change channel get sort pick remote dont need sort maybe think could maybe develop remote control moves around room dont know thats thats maybe something future talk television mean weve done research sort know cutting edge sort hand held devices lot sort use know theyre theyre mini laptops possible could devise system youre youre basically sort holding miniature computer controlling sort television stereo know buy new thing sort link well maybe click return itll get rid message hit hit return get rid message youve got cool cheers louisa user interface designer exactly sure mean tool training dont mind lets see good job got pockets today supposed right think right go think would say favourite animal cat little smiley cat would theyre independent theyre intelligent compared dogs maybe affectionate people dont think know affectionate cats look shall rub actually thats quite good check television remote lot systems kind tv video combined tv dvd combined one annoying things five remotes house youve got combined system could combined remote television supposed comes whistle think might back start grab kind slide left pull exactly clear designing rem remote mun multifunctional one decide go away work right',\n",
              " 'summary': 'The group introduced themselves and their roles to each other. The Project Manager introduced the project aim and agenda to the group. The group acquainted themselves with the meeting-room equipment by drawing on the whiteboard. The Project Manager discussed the projected production cost and price point for the device. The group began a discussion about their own experiences with using remote controls and about usability features to be included in the design. The Project Manager instructed the Industrial Designer to prepare the working design, the User Interface Designer to research technical functions, and the Marketing Expert to prepare the user requirement specification. The group discussed the function of the prototype and decided that they should restrict the remote to television for the time being.',\n",
              " 'code': 'ES2014a',\n",
              " '__index_level_0__': 0}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metric = datasets.load_metric(\"rouge\")\n",
        "metric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2P1OdkIhhfn",
        "outputId": "df77af25-f680-4a1a-fa3d-f56566645912"
      },
      "id": "b2P1OdkIhhfn",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-f7139a82fee2>:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = datasets.load_metric(\"rouge\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Metric(name: \"rouge\", features: {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}, usage: \"\"\"\n",
              "Calculates average rouge scores for a list of hypotheses and references\n",
              "Args:\n",
              "    predictions: list of predictions to score. Each prediction\n",
              "        should be a string with tokens separated by spaces.\n",
              "    references: list of reference for each prediction. Each\n",
              "        reference should be a string with tokens separated by spaces.\n",
              "    rouge_types: A list of rouge types to calculate.\n",
              "        Valid names:\n",
              "        `\"rouge{n}\"` (e.g. `\"rouge1\"`, `\"rouge2\"`) where: {n} is the n-gram based scoring,\n",
              "        `\"rougeL\"`: Longest common subsequence based scoring.\n",
              "        `\"rougeLSum\"`: rougeLsum splits text using `\"\n",
              "\"`.\n",
              "        See details in https://github.com/huggingface/datasets/issues/617\n",
              "    use_stemmer: Bool indicating whether Porter stemmer should be used to strip word suffixes.\n",
              "    use_aggregator: Return aggregates if this is set to True\n",
              "Returns:\n",
              "    rouge1: rouge_1 (precision, recall, f1),\n",
              "    rouge2: rouge_2 (precision, recall, f1),\n",
              "    rougeL: rouge_l (precision, recall, f1),\n",
              "    rougeLsum: rouge_lsum (precision, recall, f1)\n",
              "Examples:\n",
              "\n",
              "    >>> rouge = datasets.load_metric('rouge')\n",
              "    >>> predictions = [\"hello there\", \"general kenobi\"]\n",
              "    >>> references = [\"hello there\", \"general kenobi\"]\n",
              "    >>> results = rouge.compute(predictions=predictions, references=references)\n",
              "    >>> print(list(results.keys()))\n",
              "    ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
              "    >>> print(results[\"rouge1\"])\n",
              "    AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))\n",
              "    >>> print(results[\"rouge1\"].mid.fmeasure)\n",
              "    1.0\n",
              "\"\"\", stored examples: 0)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can call its `compute` method with your predictions and labels, which need to be list of decoded strings:"
      ],
      "metadata": {
        "id": "Ft4u4X9BhvJI"
      },
      "id": "Ft4u4X9BhvJI"
    },
    {
      "cell_type": "code",
      "source": [
        "fake_preds = [\"hello there\", \"general kenobi\"]\n",
        "fake_labels = [\"hello there\", \"general kenobi\"]\n",
        "metric.compute(predictions=fake_preds, references=fake_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSMJmBwNhtAv",
        "outputId": "40f538c8-461a-4380-fb86-e336eae2e920"
      },
      "id": "NSMJmBwNhtAv",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rouge1': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
              " 'rouge2': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
              " 'rougeL': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
              " 'rougeLsum': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b435a2a-171a-405f-8c57-ef4198d94cd6",
      "metadata": {
        "id": "2b435a2a-171a-405f-8c57-ef4198d94cd6"
      },
      "source": [
        "# Data Pre-Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before we can feed those texts to our model, we need to preprocess them. This is done by a 🤗 Transformers `Tokenizer` which will (as the name indicates) tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that the model requires.\n",
        "\n",
        "To do all of this, we instantiate our tokenizer with the `AutoTokenizer.from_pretrained` method, which will ensure:\n",
        "\n",
        "- we get a tokenizer that corresponds to the model architecture we want to use,\n",
        "- we download the vocabulary used when pretraining this specific checkpoint.\n",
        "\n",
        "That vocabulary will be cached, so it's not downloaded again the next time we run the cell."
      ],
      "metadata": {
        "id": "mO9eE_2Fh-2C"
      },
      "id": "mO9eE_2Fh-2C"
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_CHECKPOINT = \"t5-small\"\n",
        "MAX_INPUT_LENGTH = 1024\n",
        "MAX_TARGET_LENGTH = 128  # the notebook might crash if MAX_TARGET_LENGTH > 128"
      ],
      "metadata": {
        "id": "U5MbgiD2id9n"
      },
      "id": "U5MbgiD2id9n",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "416d3638-f922-4eec-aefc-893a2d0a19a6",
      "metadata": {
        "id": "416d3638-f922-4eec-aefc-893a2d0a19a6"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depending on the model you selected, you will see different keys in the dictionary returned by the cell above. They don't matter much for what we're doing here (just know they are required by the model we will instantiate later), you can learn more about them in [this tutorial](https://huggingface.co/transformers/preprocessing.html) if you're interested."
      ],
      "metadata": {
        "id": "HcjjQ4TIisLp"
      },
      "id": "HcjjQ4TIisLp"
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer(\"Hello, this is one sentence\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYwWwvUDipyo",
        "outputId": "b965ec21-27f9-4271-a117-7b7d73721010"
      },
      "id": "RYwWwvUDipyo",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [8774, 6, 48, 19, 80, 7142, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgNLDcRmiraq",
        "outputId": "d52352fb-453c-4ab0-e2b3-ffa528bf172f"
      },
      "id": "lgNLDcRmiraq",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [[8774, 6, 48, 80, 7142, 55, 1], [100, 19, 430, 7142, 5, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To prepare the targets for our model, we need to tokenize them inside the `as_target_tokenizer` context manager. This will make sure the tokenizer uses the special tokens corresponding to the targets:"
      ],
      "metadata": {
        "id": "3vZTTp0oi3uk"
      },
      "id": "3vZTTp0oi3uk"
    },
    {
      "cell_type": "code",
      "source": [
        "with tokenizer.as_target_tokenizer():\n",
        "    print(tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvxr8yaLi4zX",
        "outputId": "c2c92bba-4133-46e5-b3cf-1f82d4abaf57"
      },
      "id": "fvxr8yaLi4zX",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [[8774, 6, 48, 80, 7142, 55, 1], [100, 19, 430, 7142, 5, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3606: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "3673d11e-1fcb-45bf-9087-97498146bbed",
      "metadata": {
        "id": "3673d11e-1fcb-45bf-9087-97498146bbed"
      },
      "outputs": [],
      "source": [
        "if MODEL_CHECKPOINT in [\"t5-small\", \"t5-base\", \"t5-large\", \"t5-3b\", \"t5-11b\"]:\n",
        "    prefix = \"summarize: \"\n",
        "else:\n",
        "    prefix = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then write the function that will preprocess our samples. We just feed them to the `tokenizer` with the argument `truncation=True`. This will ensure that an input longer that what the model selected can handle will be truncated to the maximum length accepted by the model. The padding will be dealt with later on (in a data collator) so we pad examples to the longest length in the batch and not the whole dataset."
      ],
      "metadata": {
        "id": "1WalywMxji7n"
      },
      "id": "1WalywMxji7n"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "b5ed6249-d24d-4f44-8978-de6ca6c1a869",
      "metadata": {
        "id": "b5ed6249-d24d-4f44-8978-de6ca6c1a869"
      },
      "outputs": [],
      "source": [
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + doc for doc in examples[\"transcript\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True)\n",
        "\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            examples[\"summary\"], max_length=MAX_TARGET_LENGTH, truncation=True\n",
        "        )\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To apply this function on all the pairs of sentences in our dataset, we just use the `map` method of our `dataset` object we created earlier. This will apply the function on all the elements of all the splits in `dataset`, so our training, validation and testing data will be preprocessed in one single command."
      ],
      "metadata": {
        "id": "U5d9y9Jej7XX"
      },
      "id": "U5d9y9Jej7XX"
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_function(raw_datasets[\"train\"][:1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mfYzIYVkCHp",
        "outputId": "53b91ad9-9abe-44ac-ce86-11dbf2144ff8"
      },
      "id": "5mfYzIYVkCHp",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [[21603, 10, 3, 11191, 2913, 4378, 1464, 761, 4273, 25, 162, 1513, 18701, 3, 1092, 3, 1092, 1522, 217, 129, 640, 406, 3, 8967, 697, 762, 132, 7, 80, 737, 17, 317, 3, 3801, 3, 1092, 653, 1131, 4550, 3, 13366, 281, 4595, 3, 179, 3314, 168, 3, 1092, 3905, 107, 3, 32, 32, 107, 3, 32, 32, 107, 1513, 317, 3, 757, 7673, 15, 26, 18701, 168, 3, 122, 168, 281, 422, 422, 4595, 2586, 1416, 1327, 4595, 146, 29, 29, 32, 2087, 132, 7, 186, 15074, 2850, 263, 4595, 19126, 484, 2850, 2005, 248, 4273, 4322, 7415, 10634, 385, 422, 79, 60, 882, 614, 2785, 2087, 143, 424, 514, 2785, 10634, 711, 1681, 4273, 1843, 514, 169, 10634, 3551, 514, 169, 217, 4273, 8032, 281, 281, 4273, 2087, 228, 394, 3909, 4322, 408, 4322, 610, 1843, 3909, 133, 369, 9116, 662, 4273, 4273, 317, 243, 456, 4390, 4322, 610, 2087, 4372, 3, 3227, 129, 1219, 2904, 24, 7, 248, 269, 456, 166, 1338, 269, 8101, 166, 1338, 6786, 874, 676, 1338, 129, 29740, 921, 241, 497, 1727, 11743, 491, 9, 15303, 516, 2488, 1464, 761, 516, 515, 1321, 3376, 1464, 761, 831, 7598, 217, 6994, 17, 310, 269, 516, 372, 6171, 369, 126, 3, 52, 4322, 610, 1407, 1684, 1247, 926, 6831, 1059, 97, 800, 143, 126, 4322, 610, 1407, 1139, 2609, 1767, 80, 4166, 972, 2459, 129, 4038, 512, 698, 4038, 2417, 1573, 5679, 217, 5014, 408, 13061, 138, 408, 3117, 408, 17258, 168, 6171, 609, 53, 6553, 1407, 4677, 503, 239, 369, 394, 394, 16, 7, 17, 4028, 2459, 1574, 7489, 239, 25, 195, 796, 2888, 1019, 239, 942, 414, 62, 162, 530, 1464, 761, 653, 872, 1976, 269, 921, 7, 3, 24309, 3314, 3960, 2586, 872, 1476, 3382, 143, 417, 872, 1976, 930, 2483, 214, 9543, 281, 166, 1663, 281, 3, 89, 166, 3905, 107, 2891, 3, 9, 107, 25, 195, 888, 18701, 1861, 240, 133, 133, 3382, 2268, 982, 1243, 6553, 3679, 369, 1843, 378, 416, 2483, 217, 132, 7, 174, 132, 7, 2500, 628, 1243, 2891, 1776, 62, 162, 97, 2967, 596, 62, 162, 6229, 14120, 2161, 12746, 386, 3, 3801, 13483, 129, 6784, 7235, 5709, 39, 15, 522, 269, 168, 3, 757, 700, 186, 8636, 97, 1137, 5057, 9805, 6161, 3516, 16649, 530, 798, 79, 162, 530, 2495, 8858, 751, 17, 4410, 1256, 3314, 217, 2377, 7235, 161, 19930, 168, 6161, 80, 200, 378, 2495, 2483, 310, 240, 231, 479, 3127, 39, 15, 352, 550, 2297, 2891, 25, 162, 3, 26761, 1492, 540, 129, 1565, 2891, 320, 3, 10339, 530, 2495, 3, 26761, 474, 542, 25310, 883, 3305, 3305, 7, 1158, 1274, 39, 15, 550, 483, 387, 334, 1158, 767, 805, 2677, 685, 453, 13677, 2495, 4360, 8636, 79, 60, 731, 2453, 269, 341, 178, 269, 161, 612, 516, 6786, 874, 10186, 1644, 3014, 594, 251, 369, 1070, 2743, 479, 1789, 12012, 3, 28188, 479, 999, 1358, 1643, 13369, 985, 3983, 399, 1745, 2459, 492, 3199, 6346, 168, 700, 3199, 6346, 6865, 39, 15, 3, 13366, 15998, 7, 796, 1358, 240, 428, 3199, 6346, 399, 1745, 3345, 15998, 1358, 2082, 186, 3173, 479, 1789, 516, 53, 1789, 500, 97, 351, 4322, 610, 166, 912, 126, 4322, 3382, 479, 3071, 500, 97, 199, 5265, 408, 126, 825, 3376, 6171, 479, 479, 1407, 6268, 2459, 751, 17, 129, 6780, 1153, 479, 1407, 378, 243, 514, 169, 169, 217, 3, 210, 6171, 3, 1092, 129, 223, 1330, 11743, 1137, 269, 120, 243, 132, 7, 1327, 16241, 386, 662, 1904, 19886, 15, 26, 562, 1407, 4322, 7, 1254, 3, 52, 8543, 120, 3839, 54, 17, 3, 189, 80, 1407, 10634, 1137, 241, 241, 16538, 168, 241, 25851, 3, 179, 169, 7096, 7, 17, 97, 241, 269, 120, 243, 80, 4322, 1077, 8543, 120, 3839, 931, 228, 9409, 351, 338, 1904, 1126, 1904, 151, 129, 261, 338, 4322, 7, 2459, 79, 60, 5834, 2459, 143, 6446, 97, 1550, 394, 3909, 168, 62, 162, 530, 874, 676, 414, 1338, 456, 2943, 53, 416, 1338, 12010, 676, 269, 62, 162, 530, 3, 23, 26, 369, 213, 7, 281, 24, 7, 479, 269, 62, 162, 530, 1681, 2817, 269, 269, 8032, 62, 162, 530, 464, 408, 3, 23, 26, 3, 76, 23, 26, 2268, 3621, 408, 1070, 1139, 5971, 16726, 806, 3909, 1622, 568, 525, 3763, 964, 7233, 479, 942, 416, 12010, 676, 3382, 3, 1092, 653, 1431, 676, 1338, 428, 416, 1338, 4390, 4322, 610, 24, 7, 1176, 1137, 503, 239, 429, 143, 3055, 3, 390, 251, 4677, 133, 483, 352, 500, 97, 317, 39, 15, 269, 1522, 143, 3, 17, 208, 15407, 1049, 1733, 3, 1092, 676, 168, 217, 985, 1781, 269, 3, 5840, 77, 1070, 2743, 146, 29, 29, 32, 18701, 7, 2483, 214, 829, 1076, 545, 1753, 269, 21820, 3, 13366, 281, 1782, 3, 13366, 3314, 80, 14621, 168, 1416, 352, 3, 11691, 7, 107, 1106, 424, 269, 132, 7, 1782, 3887, 79, 60, 14020, 79, 60, 373, 1095, 5604, 5604, 39, 15, 1829, 1843, 720, 720, 7718, 79, 60, 373, 1107, 79, 60, 373, 882, 2787, 373, 418, 694, 1782, 79, 60, 92, 207, 2510, 168, 1843, 9, 129, 1843, 9, 470, 129, 7718, 79, 60, 7718, 79, 60, 882, 5295, 168, 24, 7, 3887, 248, 3, 757, 80, 378, 435, 512, 585, 151, 557, 129, 11319, 381, 10634, 168, 1137, 132, 7, 882, 557, 1995, 1995, 1664, 1843, 4322, 7415, 9589, 1730, 39, 15, 3, 7, 144, 3533, 4322, 5775, 1307, 562, 3, 10339, 657, 25, 26, 129, 483, 4245, 129, 1843, 1432, 4322, 2483, 174, 1843, 2087, 317, 228, 2087, 1344, 4322, 610, 6914, 300, 562, 2483, 214, 24, 7, 24, 7, 2087, 424, 647, 1350, 4390, 1243, 62, 162, 612, 585, 1843, 214, 3753, 3023, 1843, 609, 1213, 1904, 418, 1843, 169, 214, 79, 60, 79, 60, 3016, 4544, 7, 487, 228, 13282, 15, 358, 39, 15, 39, 15, 6171, 1843, 3609, 20955, 1218, 14498, 1843, 4390, 16687, 214, 805, 126, 589, 1843, 1309, 168, 2087, 1214, 1205, 34, 195, 129, 5413, 1569, 1560, 1560, 1205, 129, 5413, 1569, 25, 162, 530, 1633, 11288, 7, 16585, 159, 9, 1139, 3459, 4378, 1776, 417, 1243, 1464, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[37, 563, 3665, 1452, 11, 70, 6270, 12, 284, 119, 5, 37, 2786, 3440, 3665, 8, 516, 2674, 11, 8101, 12, 8, 563, 5, 37, 563, 29740, 1452, 28, 8, 1338, 18, 3082, 1277, 57, 5364, 30, 8, 872, 1976, 5, 37, 2786, 3440, 5172, 8, 16037, 999, 583, 11, 594, 500, 21, 8, 1407, 5, 37, 563, 1553, 3, 9, 3071, 81, 70, 293, 2704, 28, 338, 4322, 7415, 11, 81, 178, 2020, 753, 12, 36, 1285, 16, 8, 408, 5, 37, 2786, 3440, 24088, 8, 9199, 11296, 12, 2967, 8, 464, 408, 6, 8, 6674, 25064, 11296, 12, 585, 2268, 3621, 6, 11, 8, 4329, 8865, 12, 2967, 8, 1139, 5971, 16726, 5, 37, 563, 5172, 8, 1681, 13, 8, 14402, 11, 1500, 24, 79, 1]]}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "fc9c26ef-1043-4270-bb51-fd4f01b9fd46",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "c76bc0041a414f6faaad443f96be696b",
            "3093b562cc0d4877b2b05bad1d829008",
            "21514d73f54849caaf01a92b767fe065",
            "3344ea41fb744f5d8aef9311a2e78b99",
            "51637d2805d04469bdb99abdac9decaa",
            "cbd11ccb6a604566aca0aa1f9ea7d7e6",
            "64be13c57ab84a96bdbc88b44bf9c4d7",
            "0cb435c23f0145a39e08b81bb362029b",
            "f68ef77c4bf848f497abd43de90dca1a",
            "e713475c78d54244bc04786744ad53a4",
            "4daabcadd57e4ef9b9009061710be395",
            "93c83cde52cb4d0ab420dc9c9dcb1d29",
            "272399d1097e4a16a3c013a636b40019",
            "a6c7050c742d407e8b096c9672135368",
            "ed409ffbdcaa4280acd7e6eac7788930",
            "69551e3b032f443d89615435bdf36d8e",
            "19ff556d873845a1843ca0981be4b6e3",
            "99c6f6a912e64682ba4a8d6f4e6d5db0",
            "174e7f460f3c4754b893a474bba8cf31",
            "8ec87c74102b4d8790c8477b9699df9c",
            "94b598e10455409291fc6899aa1cf7d1",
            "e38311aaffa8407d97f31ce391043ebe",
            "abc65ccc758f4268ac61a1522f6fe66d",
            "c7894d279695404d9b7e286001c0460d",
            "eb090d25112f4b5dba83d266e5527aa7",
            "2e4c45040206446c9e48bc3735aa548c",
            "a33afbfd764f492a951529c2de408579",
            "614b818bc469417f801f84aaac35801f",
            "1454e2f9cef5437a9d60dfb83d5fb520",
            "c09cd61fdc63402abb20eef3d875314e",
            "54868df9eb8745078b1834442fd936ac",
            "c3b1492792594f22a45764709f11c951",
            "1c5cedd4cd5749259530cc5fc0aac76b"
          ],
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "fc9c26ef-1043-4270-bb51-fd4f01b9fd46",
        "outputId": "05353e05-0a23-4110-a87a-e280cd37f874"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/120 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c76bc0041a414f6faaad443f96be696b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/22 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "93c83cde52cb4d0ab420dc9c9dcb1d29"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/29 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "abc65ccc758f4268ac61a1522f6fe66d"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Note that we passed `batched=True` to encode the texts by batches together. This is to leverage the full benefit of the fast tokenizer we loaded earlier, which will use multi-threading to treat the texts in a batch concurrently."
      ],
      "metadata": {
        "id": "rOGnPz9jkY2q"
      },
      "id": "rOGnPz9jkY2q"
    },
    {
      "cell_type": "markdown",
      "id": "6e8e7adf-29af-4d7a-95d8-8c94277beda3",
      "metadata": {
        "id": "6e8e7adf-29af-4d7a-95d8-8c94277beda3"
      },
      "source": [
        "# Fine-Tuning the model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that our data is ready, we can download the pretrained model and fine-tune it. Since our task is sequence-to-sequence (both the input and output are text sequences), we use the `AutoModelForSeq2SeqLM` class. Like with the tokenizer, the `from_pretrained` method will download and cache the model for us."
      ],
      "metadata": {
        "id": "eDQcWZUVkzk4"
      },
      "id": "eDQcWZUVkzk4"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "8af1e4f4-dc94-4960-8484-a97371a9fd13",
      "metadata": {
        "id": "8af1e4f4-dc94-4960-8484-a97371a9fd13",
        "outputId": "7cd1213b-38a4-42c8-c895-9361b73dba4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
        "\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we set some parameters like the learning rate and the `batch_size`and customize the weight decay. "
      ],
      "metadata": {
        "id": "wCrNbJt-k83X"
      },
      "id": "wCrNbJt-k83X"
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 8\n",
        "learning_rate = 2e-5\n",
        "weight_decay = 0.01\n",
        "num_train_epochs = 1"
      ],
      "metadata": {
        "id": "dYWXBsOUlFvZ"
      },
      "id": "dYWXBsOUlFvZ",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we need a special kind of data collator, which will not only pad the inputs to the maximum length in the batch, but also the labels. Note that our data collators are designed to work for multiple frameworks, so ensure you set the `return_tensors='np'` argument to get NumPy arrays out - you don't want to accidentally get a load of `torch.Tensor` objects in the middle of your nice TF code! You could also use `return_tensors='tf'` to get TensorFlow tensors, but our TF dataset pipeline actually uses a NumPy loader internally, which is wrapped at the end with a `tf.data.Dataset`. As a result, `np` is usually more reliable and performant when you're using it!\n",
        "\n",
        "We also want to compute `ROUGE` metrics, which will require us to generate text from our model."
      ],
      "metadata": {
        "id": "UWVih_4blNWO"
      },
      "id": "UWVih_4blNWO"
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\")\n",
        "\n",
        "generation_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"np\", \n",
        "                                                  pad_to_multiple_of=128)"
      ],
      "metadata": {
        "id": "8fOgRZQAlM8g"
      },
      "id": "8fOgRZQAlM8g",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we convert our datasets to `tf.data.Dataset`, which Keras understands natively. There are two ways to do this - we can use the slightly more low-level [`Dataset.to_tf_dataset()`](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.to_tf_dataset) method, or we can use [`Model.prepare_tf_dataset()`](https://huggingface.co/docs/transformers/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset). The main difference between these two is that the `Model` method can inspect the model to determine which column names it can use as input, which means you don't need to specify them yourself. Make sure to specify the collator we just created as our `collate_fn`!"
      ],
      "metadata": {
        "id": "-CEtaL0Cm3G_"
      },
      "id": "-CEtaL0Cm3G_"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "4e096858-6232-4cba-9ba1-b479d0cbdf2a",
      "metadata": {
        "id": "4e096858-6232-4cba-9ba1-b479d0cbdf2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45336e54-f375-4b23-f3ac-91bfb157a329"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        }
      ],
      "source": [
        "train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n",
        "    batch_size=batch_size,\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "val_dataset = tokenized_datasets[\"val\"].to_tf_dataset(\n",
        "    batch_size=batch_size,\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "generation_dataset =  tokenized_datasets[\"val\"].to_tf_dataset(\n",
        "    batch_size=batch_size,\n",
        "    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n",
        "    shuffle=False,\n",
        "    collate_fn=generation_data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d72a0bc5-3c42-4e36-83b2-05c0b4716eb0",
      "metadata": {
        "id": "d72a0bc5-3c42-4e36-83b2-05c0b4716eb0"
      },
      "source": [
        "Now we initialize our loss and optimizer and compile the model. Note that most Transformers models compute loss internally - we can train on this as our loss value simply by not specifying a loss when we `compile()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "ece28870-3915-4e38-aa8c-2a6c062414b2",
      "metadata": {
        "id": "ece28870-3915-4e38-aa8c-2a6c062414b2",
        "outputId": "20a5d717-4481-48fa-de34-1af7dc75cbb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AdamWeightDecay\n",
        "import tensorflow as tf\n",
        "\n",
        "optimizer = AdamWeightDecay(learning_rate=learning_rate, weight_decay_rate=weight_decay)\n",
        "model.compile(optimizer=optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de049551-b02e-4efa-a063-1bfbbd2c3a3f",
      "metadata": {
        "id": "de049551-b02e-4efa-a063-1bfbbd2c3a3f"
      },
      "source": [
        "### Training and evaluating the model\n",
        "\n",
        "Now we can train our model. We can also add a few optional callbacks here.\n",
        "- TensorBoard is a built-in Keras callback that logs TensorBoard metrics.\n",
        "- KerasMetricCallback is a callback for computing advanced metrics. There are a number of common metrics in NLP like ROUGE which are hard to fit into your compiled training loop because they depend on decoding predictions and labels back to strings with the tokenizer, and calling arbitrary Python functions to compute the metric. The KerasMetricCallback will wrap a metric function, outputting metrics as training progresses.\n",
        "\n",
        "The KerasMetricCallback callback takes two main arguments - a `metric_fn` and an `eval_dataset`. It then iterates over the `eval_dataset` and collects the model's outputs for each sample, before passing the `list` of predictions and the associated `list` of labels to the user-defined `metric_fn`. If the `predict_with_generate` argument is `True`, then it will call `model.generate()` for each input sample instead of `model.predict()` - this is useful for metrics that expect generated text from the model, like `ROUGE`.\n",
        "\n",
        "This callback allows complex metrics to be computed each epoch that would not function as a standard Keras Metric. Metric values are printed each epoch, and can be used by other callbacks like `TensorBoard` or `EarlyStopping`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "\n",
        "\n",
        "def metric_fn(eval_predictions):\n",
        "    predictions, labels = eval_predictions\n",
        "    decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    for label in labels:\n",
        "        label[label < 0] = tokenizer.pad_token_id  # Replace masked label tokens\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    # Rouge expects a newline after each sentence\n",
        "    decoded_predictions = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_predictions]\n",
        "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "    result = metric.compute(predictions=decoded_predictions, references=decoded_labels, use_stemmer=True)\n",
        "    # Extract a few results\n",
        "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
        "    # Add mean generated length\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "ZrcfEJwypKsj"
      },
      "id": "ZrcfEJwypKsj",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now we can try training our model. By default, we only do a single epoch of training here, as the inputs are very long, which means training is quite slow. However, you may wish to experiment with larger pre-trained models and longer training runs if you want to maximize the quality of your summaries."
      ],
      "metadata": {
        "id": "f5GeSPNZqT_o"
      },
      "id": "f5GeSPNZqT_o"
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THG92InRqfR-",
        "outputId": "f9eb38d9-88a2-447e-eb85-5758d8e73c06"
      },
      "id": "THG92InRqfR-",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.keras_callbacks import KerasMetricCallback\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "tensorboard_callback = TensorBoard(log_dir=\"./summarization_model_save/logs\")\n",
        "metric_callback = KerasMetricCallback(metric_fn, eval_dataset=generation_dataset, \n",
        "                                      predict_with_generate=True, use_xla_generation=True)\n",
        "callbacks = [metric_callback, tensorboard_callback]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1WJxn9gqftg",
        "outputId": "f666ef12-7af6-4929-a2d2-ed74ba1d34b8"
      },
      "id": "_1WJxn9gqftg",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:No label_cols specified for KerasMetricCallback, assuming you want the 'labels' key.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_dataset, validation_data=val_dataset, epochs=num_train_epochs, callbacks=callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M37LhhV8qjEu",
        "outputId": "d04c5d0f-5236-4c99-deaf-b06f700fdcc1"
      },
      "id": "M37LhhV8qjEu",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "15/15 [==============================] - ETA: 0s - loss: 5.9003  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py:371: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  return py_builtins.overload_of(f)(*args)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r15/15 [==============================] - 1383s 89s/step - loss: 5.9003 - val_loss: 5.4759 - rouge1: 5.8999 - rouge2: 0.7707 - rougeL: 4.6196 - rougeLsum: 5.4657 - gen_len: 19.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fcb63b90ca0>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba6a1175",
      "metadata": {
        "id": "ba6a1175"
      },
      "source": [
        "# Inference\n",
        "\n",
        "### Pipeline API\n",
        "\n",
        "Now we will try to infer the model we trained on an arbitary transcript. To do so, we will use the pipeline method from Hugging Face Transformers. Hugging Face Transformers provides us with a variety of pipelines to choose from. For our task, we use the summarization pipeline.\n",
        "\n",
        "The pipeline method takes in the trained model and tokenizer as arguments. The framework=\"tf\" argument ensures that you are passing a model that was trained with TF."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = raw_datasets[\"val\"][0][\"transcript\"]\n",
        "sum = raw_datasets[\"val\"][0][\"summary\"]"
      ],
      "metadata": {
        "id": "-GKnQJtJ1Ik7"
      },
      "id": "-GKnQJtJ1Ik7",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, framework=\"tf\")\n",
        "# summarizer = pipeline('text2text-generation', model_name, framework=\"tf\")\n",
        "\n",
        "summarizer(doc, min_length=10, max_length=MAX_TARGET_LENGTH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meuKu4Bi1K4U",
        "outputId": "4f92e7f8-92be-445c-9e7b-1a0195627f7a"
      },
      "id": "meuKu4Bi1K4U",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'summary_text': 'names tara user interface designer also responsible functional design phase conceptual design phase user interface design elk vicious sheep really good quite good think favourite animal would dog really sure draw one ive never drawn dog dont think tempted draw snail cause draw sometimes theyre really easy draw right gonna really funny dog cause sure draw dog suppose lon god right yous know supposed dog dogs theyre good humans trained police dogs . theyre friendly animals dont look yous find theres lot buttons remote control dont know half dont know nothing welcome meeting everyone gonna attempt make'}]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "ILoZwc951PRb",
        "outputId": "bc1d3adf-e35f-4429-f49d-3cfdcc92b70d"
      },
      "id": "ILoZwc951PRb",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The project manager opens the meeting by introducing herself and asking everyone to say their name and role in the group. She then states the agenda of the meeting and tells them that they will be designing and creating a new remote control that should be trendy and user-friendly. The meetings will focus on functional, conceptual, and detailed design. Next, each group member draws their favorite animal on the whiteboard and explains the characteristics of that animal. After that the project manager covers the project budget, and then they begin discussing their personal experiences with remote controls and how they want their remote to look. Then the project manager closes the meeting by telling each group member what to do in preparation for the next meeting.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = raw_datasets[\"val\"][1][\"transcript\"]\n",
        "sum = raw_datasets[\"val\"][1][\"summary\"]"
      ],
      "metadata": {
        "id": "NCX9QF_a6eis"
      },
      "id": "NCX9QF_a6eis",
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, framework=\"tf\")\n",
        "# summarizer = pipeline('text2text-generation', model_name, framework=\"tf\")\n",
        "\n",
        "summarizer(doc, min_length=10, max_length=MAX_TARGET_LENGTH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVnBx5dP6eY7",
        "outputId": "467e94c9-30b8-4a69-9709-901a046533d1"
      },
      "id": "RVnBx5dP6eY7",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'summary_text': 'uhoh maybe full page spent lots time talking about remote control . sammy benjo expert design artist thats gonna less technical functions user interface current intentions everything linked next slide .'}]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "id": "B9_pRnYE6h-p",
        "outputId": "b7fe134f-064f-455e-faf0-bf1cb80e493b"
      },
      "id": "B9_pRnYE6h-p",
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The project manager opened the meeting and stated the agenda to the team members. The marketing expert discussed the findings of a survey which indicated that current remotes are ugly, difficult to use, have a number of unused buttons, frustrate users when misplaced, and contribute to RSI. The marketing expert also stated that young users like speech recognition and that users in general want buttons for power, channel selection, volume control, and a few lesser used settings. The user interface designer presented existing remotes to exemplify the need for simpler designs, discussed the use of components such as titanium and a back-lit LCD screen, and discussed other features to consider such as color options. The industrial designer discussed the interior workings of a remote and how to handle universal capability and speech recognition. After the project manager's closing, the project manager recapped some decisions and the team discussed how to handle the issue of locating a remote when misplaced.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = raw_datasets[\"test\"][0][\"transcript\"]\n",
        "doc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "id": "lyFVSqTPyER9",
        "outputId": "629c7c98-3c30-4474-9434-9558ac2c1680"
      },
      "id": "lyFVSqTPyER9",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'get seven ten one bad think topic box based whats called nom nite text area something text area sort highlighting stuff probably look sure inbuilt though honest wh wh dont ill w thinking going direction anyway see mean think think already calculated data supposed right come back numbers whatever give think mean whenever open window one speaker sh pretty much would thought definitely one present although could th could one highlighted suppose mean would would implementing said wanna see per topic wanna see one spoken something could could zap topics meetings sorry would come much spoke pick one looks interesting wanna definitely random things return talking time good mean put topic spoken well could use topic pop window thatd r wasnt moment rightclick topic window one topics option pop popping open window gives list meetings containing topic would would calculate would summarisation populate know global right sta stored together right sounds good done yet gotta decide wanna definitely usable tonight see well let know sort usable least mean picture mean gonna take long mean sounds good stored home directory sounds good well wanting write directly wanna wanna change code youre wanna change original browser file done actual one copy well tell usually usually work copy day update without checking th one one shared one changed let know cool youre working rip present results nice overall mean hi highlighting stuff actually n text area whatever defined ha handy highlights youve got different different highlights youve got look wrote theyve got user highlights selection highlights time highlights something else quite easily text area called area area dot set highlights something know something thats way highlight crosshighlight topic transcripts sorta thing really really allocates dat room whatever memory talk spent much time front think right well ive ive random popup windows speaker characterisation good feel windows lots fiddling well know pop popup window difficult enough basically set whatever ten popup windows memory already initialize theres might ten speakers different mee windows working click topic see list topics meetings topic mentioned question said wanted start screen kind start screen want mean want general blurb welcome browser something animation dancing want maybe could write whatever called top window saying welcome meeting browser mean moment way drop menu youve seen dialogue class use mean think two options either load meeting choose search youve got results click something loads first ju bec get nom object suppose wouldnt wanna use wouldnt wanna use nxt beginning anyway dont wanna search locally wanna search globally lets say well would open meeting first right lets say two options either pick one meeting search globally find meeting want default mean know local search cant global search without anything pretty much mean local global inverted file search gives nom objects basically local search right well th thats always gonna case search mean know option search way global search first first step return meetings theyre even meetings name meetings know say wanna search word language gives result meetings contain word language decide well wanna search meeting meeting definitely one papers names better understanding whatever even w one one even even better understanding something love love ive ive put already sort concretely startup startup libraries window well mean think would time well wha tfidf stand ah general score would whole language whole corpus well know might general frequencies see would fake topics would would look bunch list three words something would really handy actually test user opposed using cause segmentation thats great cant cant compare hand annotated know hand segmented tool would work thatd really nice definitely thats thinking well want want sort main idea speed speed search nxt whole corpus feasible would general inverted file would nice extra wasnt sure true true interim mean time well thatd really nice cause mean would give searched word would give meetings also often w word occurred meeting would useful exactly would useful know c search language return know basically meetings wanna see actually spoke mentioned ah language edinburgh language ad add something would combine thats question come back startup screen unsure look mean wanna part choose choose meeting kind information wanna meeting mean u longer name obviously one cause dont longer name users spoke users took part thats probably pretty much group sure give idea general idea pick meeting w kn wanna look meeting soandso spoke know sort sort research group want first meeting look dont think need wanna way soon theyve got loaded first meeting browse quite nicely mean w gonna know user model would defy whole point browsing right wanted look five meetings well scroll list find favourite one mean come back every day browsing lets see whats changed today right well thats w wasnt gonna put know help sort explanation browser works honest wasnt right see mean main thing wanna view meeting right search happens search basically basically either user wa knows meeting wants look clicks doesnt searches one sort looks see mean inside search menu thats thats looks right unsure put information need dropdown menu want information name longer name sp users codes mean least maybe know codes corpus corpi dropdown definitely said saying pick user come list extended list meetings know bdr whatever called one gives longer name blah gives know participants thinking tool tip think tool tip ive tried yet gonna keen sure th annoying hold know ha yes lot dont know search u pecif pec specific user hold mouse one wait pop hold mouse one wait pop itd nice basic information meetings thinking updates depending youve got highlighted maybe thats best idea youve got dropdown menu search search button search go button sort field updates dynamically depending youve got highlighted sounds good think nom nom actually stand nite yes youve got lots nom something nite something n something really descriptions hints saying youve gotta update jonathan uni unintelligible great know definitely fun right anyone else got anything say want another meeting end week pernilla something mean think gonna quite bit work week ill probably talk think well well disc discuss friday one friday time ive got doctors appointment ten two li w qui cant would give us thirty five minutes something well quite late know five six well three oclock lets say three oclock right ill write ill write meeting ill send pernilla well shes collect meeting summaries probably end depending r repeat briefly weve spoken weve din done progress speaker characterisation speaker characterisation weve mentioned topic labelling keywords discussed search ranked michael anything else startup window well basically unless youve got time end would nice end think would think think w would would give relevant result things search language might topics know n meetings topic language specifically speak language use word every sentence youve got mention know mention briefly twice saying know language german something stupid dont really want wanna distinguish somehow would nice anyway multiple terms something really simple mean disregard mean youd youd really simply know youll results discuss friday doesnt intelligent exactly well many meetings sh shouldnt really wow ooh thats true would useful well word language might occur wanna find one actually contains usefully somehow know still wanna able search word language sensible way well well discuss friday tick oops squeeze aye right going sure said things said getting along wanted talk actually speaker data processing fine dont particularly want b b gui really someo wanna tell want data presented cause want tell cause moment ive created two classes one represents speakers one represents meetings meet information contained within object wr writes objects objects contain information meetings speakers speakers meetings amount speak averages contained speakers theres two separate class arent theyre two different objects recall write writes objects call objects back ha returned objects information need call methods return whatever want everything thats wanted know ho whats easiest way data supposedly calculated stored objects dot object files means ret call constructor call load thing call create list vector whatever wanna call one time populate window thats wanted format want data come back cause come back almost anything whats easiest display screen ca could returns cause moment main data structures hash tables meetings say got one says percent called percent talk ones percent noise one percent participation see mean got inside got link got got n speakers name got percentage thing either come back hash table returned vector say noise string noise w x percent say vector cou dont know whatever w either th could either could embedded vector array strings one represents one person whatever easiest thing dec wanna display exactly exactly see thats ca look format speaker speaker speaker controlling thing thats calculated well stored speaker objects thats thats easy quite amusing actually influences lived germany six months dont know effect spend much time talking brits bizarre thing people come thats calculated well get dialogue acts dont think thatll difficult box populated populated one present one highlighted one present cause moment using methods say using talks thats says get get talk time takes name w could call would call call meetings method said return would populate easy thing comes meetings would easy well ill w ill leave lots methods st thatll return one number time thatll easiest way thats fine get talk time think called moment something stored object file processes whole lot offline stores object theyre much much smaller theyre one one thou one k preprocessed method objects got return bunch return methods g recreate object got load method call null constructor cause call th proper constructor meeting goes processing stores object call null constructor pr call load call load whatever one anything go list one object meeting although although c tell tells participated moment tells participated amount participated percentages time well thatll pro thatd easy wont difficult would would cause problem anything wasnt annotated topics topic specified default default actually lot people dont get ch stuff right wouldnt problem could search meetings objects cause thats thing small load whole lot search whole lot find problem topics doesnt crash thing global statistics come straight dont cause theyre meter met th speaker class knows stuff meeting class knows stuff dont think thatll difficult want thought stuff important anyway first design gui first cause w problem change classes objects serial numbers change cant reload object processing done havent quite finished would become synch get bit funny gave one gave one worked changed run thing wouldnt ever able load objects back youd youd v multiple copies objects place itd get silly think wanna make picture c without anything presuming ju text box cause doesnt well wont take long get finished think ill need done stuff first cause otherwise objects wont home directory havent global yet finished global otherwise would get confusing way doesnt crash try load ten one crashes doesnt bit dumb fool c c load ten different si engines simultaneously fine cant cant thinks one cause amount kinda call new class fine dont wont l says ive got space use otherwise goes talks say nice machine goes theyre done cool pop welcome ls nlsd browser speech music drum rolls switchboard comes thats blank form buttons load meeting load search load something else whistle tune thats cheaper xt search would need meeting loaded start searching doesnt cant thing search nite object model time get one youve loaded observation doesnt way c cou could use inverted file search return list meetings use one load search wont observation even go se go search whole corpus start something bizarre reason engine theres one theres one search method search engine engine class one search method globally doesnt take long load anyway load dumb one doesnt exactly gives one time anyway doesnt cause otherwise youll crash thing slow though thing shouldnt bad dont think much problem make strings long possible return things actually needs search needs loaded think better names meetings retranslate well thats well use well thats whats thats working group wanna see meetings even better understanding thats cool thats good would idfs mean dfs document frequencies word corpus steves talking topic labelling somebodys done keywords g idfs dfs already would cant better search without tfidf think need amount occur documents basically amount df document frequency amount word occurs term frequency amount occurs ea one amount occurs document one amount g occurs generally occurs specific documents compared compared general score bet informative certain corpus corpus still data plus stop list remove stuff doesnt ta gonna prob basically equal score massive bunch keywords thatd probably easiest thing keywords keywords three f three five words documents term frequency inverse document frequency java class something dont know whether itll work also keywords gives whole new type search keyword search could keyword search could topic search thing instead would search keywords tell topics actually get searching keywords see mean suppose even calculating w whatsitsfaces would much long easy bit probably easiest calculate based upon whole occurrences corpus calculate per topic cause dont integrate much information search without tfidf cant rank search thats isnt idea first place rank rank results wont slow ranking wont slow still uses inverted file ranks results amount higher thought part doesnt matter guess thats part dont worry doesnt cause gonna ive got time anyway well itd give rank would whole point say top one bottom say informative tfidf informative score isnt depends treat compound nouns compound noun sunny day adjective simple form would separate rank one term could make complicated make th add em dont wanna start looking bo guess sum individual tfidf term returned generally bit crude give score higher informative term thing would give thing pretty crude anyway looking gonna look six separate c cause gonna go nxt search return isnt thats true thats less crude isnt groups terms without word pairs omission dont know works thats remem idea gives informative score combine guess theres lots literature theres lo whole load manning schutz theyve got whole chunk ir isnt basic information retrieval theyve got big good chapter havent got cognate hundreds pdfs want startup screen cause guess guess thats useful save preferences could save preferenc preferences guess well call favourites favourites enough information would quite good b search buttons doofus mode search even two things said one sai one said take straight meeting textbox enter drop menu another said search loaded instantly loaded search screen one one thing search default opens search window opens thats interface go brings browser f searched something search search window could something said drop menu says go button said take f f thing comes youre finished start one window got search stuff search go wherever go button takes wherever else wanna go topic says welcome welcome browser drop dropdown menu wanna users theyre always nonsense thats true good wanna search search wanna look one meeting look thats fine thats thats true thats unless two one one one one got two one meeting one speak choose go go go go go go go youre searching could microsoft stylie hold pops thing complicated mouse isnt something actually n lets get bloody way trying search damn thing thats true thats annoying thats could list users say wanna look user go find find pulls list ones got user search didnt search maybe leave dont worry speakers theyre speakers theyre search quick access f text box thats got go doesnt get way full name speakers without even ever going anywhere near loading think cool thing meeting nom think right object model sure youre right right corpus right corpus got right elements got right attributes think n ones interfaces nite one one ways round ones interface ones actually implemented class cause go back enough whats name good api theres lot description crude tells end saying returns n text box whats doh implementation n text interface whats extended version stop might useful mightnt talk next week sp two dont mean straight could quite good right friday morning three oclock dont know maybe sure weekend two say three oclock theres v serious problem ill tell might th dont think need probably threes better five six three say three oclock theres problem three oclocks problem five six problem cause wont dont think wont sure collecting youre sending mean collecting right sorry school gifted well thought somebody collecting right well thats thought said wed know whos missed whos weve done would help lot single terms would useful multiple terms unless wanna something way multiple terms could override ca could ignore ranking doesnt show together could perhap could penalize could put b weight nxt doesnt show together either disregard put weighting g many pairs get next might talk pernilla cause things youre gonna get lot results one got happened bottom actually relevant one something would push highest exactly also something sunny day sunny day arent mentioned together lot sunny happens mentions term low push one combine dup add together ranking system moment somethings amazing w highly ranked thing tfidf could ignored falls bottom assuming return results type language returns seventy five meetings already seventy f seventy five meeting return seventy five wh stop rank something returns twenty even returns twenty cut ten rank whats threshold something would look one time done tick ive signed already wanna take look comments theres excellent right see ya one thing wondering standard control use connect different things youre topic goes highlights topic text whatever particular type control one nxt sorry nite controls nom havent build new windows sort thing topics well gonna one speaker guess rest right well talk speaker characterization accessing list speakers right actually know might also help talking architecture things gonna plug mean things modular mean would good idea used idea anything else else actually even searches whatever know sort thing mean search results theyre predictable thing name certain properties list properties vector whatever right h matter deciding would easiest way around mean right strange right time going straight xml files information right well see see right right preprocessed thanks right right thats cool right right entire corpus individual ones one fi one object thats nice meeting wonder would useful stuff well right return null blank blank string something right good idea actually upload stuff well make one change file load one instead default two lines code mean well one thing thats action loading search thing well version yes directory copy yes going update one cause makes sense test right right update havent havent made changes yet ill make one ill let know cause say two lines code anyways play theres much yet get better results better presented wire topics summaries thats pr well also well wanna find kind objects ones goes highlights cause thats know right sounds good things pernilla gets index thats gonna kind fun wondering well way youre doi gonna ask way loading new corpus time meeting youre objects dont need cause inverted search says theres know ten documents gonna load ten corp corpora individual next nxt search think probably serious thats thats insane right right right exactly doesnt take long debugging well probably getting sick something sort guides obvious things right right gonna ask assume wanna load meeting first necessarily guess theyre search entire corpus kinda makes sense right well put logic could probably even shouldnt hard put check hasnt loaded force load one unless know meeting well hard well things may n looking word whate know shows ten different meetings point theyll probably wanna want dumped default goes first one want number five ones returned mean go well load thing think loads itll load transcript window whatever window right think topic ones although time probably caused loading actual data thinking kinda saying nom object right mean search right thats true global one feeds local getting usable data gonna search files gonna loading one time get data wanna right could time consuming ten documents hit thing well j thinking way could know st cash results nice little format thatll make things bit easier thing mean weve got gonna needing really really load entire corpus meeting trying show us trying highlight text transcript whatever gonna data time search window know ten different meetings know word wireless comes go meeting one reload nom object next one mean thats gonna could right always seems slow loading first time least thats ive lately sort loading test try something else shut load sorta based doesnt take long time f right thats true makes sense way know specifically meaning thatll save time sure dont wanna extra loading time either could avoid could check mark ones wanna check well think text probably sort bdb know string long names working group part probably even might right see accurate actually need frequencies right right well thing done theres trying get work friday sorta built trouble getting running properly sort standard sort stuff w last heard wasnt working gonna look know straightforward sorta thing really fulfils need need need frequencies dont know well wouldve list whats stand well corpus probably probably term frequency general one pr right actually whats tfidf really uhhuh right well need mean looking stuff guess topic dont think document exactly know right b nice right well dont know actually cause thing mean typically th thing looking happens meeting theyre looking particular term something wanna know term exists exist wanna see know two word term tfidf handle compound noun two words together f dont know dont know something edinburgh university well essentially dont know sunny day know lets make complicated thing thing way looking words period word index well thats way thinking saying two words documents go closer nxt search look exact term regular expression whatever right choose list meetings wanted search wondering gonna give us something cool absolutely sort nxt search still gotta run find terms know patterns also wild cards work something youre looking wireless wired wire blah blah blah wire star actually thats problem pernilla cause actually work sure thats bad ive got somewhere pdf pdfs switch postscript switch back pdf dont know thats might good thought sort dis know mean attended p probably part mean therell set people subset meeting probably search stuff start working browse search browse youre looking something speci youre thinking know exactly wanna go wanna go know meeting december third wanna go see stuff immediately go browse right browse meeting browse blah browse speaker thinking different ways different buttons top theyre looking particular person particular working group particular whatever mean could break things level detail want could keep general thinking initially load range things theyd wanna theyre part x working group theyll wanna get wanna save preferences well theyre part bdb working group theyre gonna wanna look bdb ones wed default dont know based last time right thats right exactly forget right well getting back sorta could startup screen range things could could search options browse options mean dont know playing around ideas tie evaluation tasks say well want search meeting search user particular meeting blah blah blah sort know guide hey task theres button p perfect know know dont know thats suggesting saying lot programmes well kind lame programmes sort thing first cause dont want people go menus search commonly used tasks exit let use programme know well probably search something find specific meeting know thats easy sort thing fits criteria well dont actually user names arent codes ami eleven whatever case want dropdown dont want type allow multiple youre looking one specific speaker right speakers tool tip cause thats true thats nice easy right think case wr write corpus writable corpus something nxt something right right area drove crazy god really gonna keep going friday afternoon something wont meeting steve tuesday wanna arrange get one earlier maybe three dont know thats true true sure possibly dont know usually need break although well dont know cause lab try get work done lab stuff ten two th three oclock wanna get get away right cause think shes got class eleven weve got cla well three us class twelve could go could try one could try shortly well yea well well wouldnt long meeting would wanna probably ones bad wont right right theres sorta basic implementation stuff wondering fine progressing decide whether ranked cause well kinda think would complicate things quite bit bring us lot independently words particular document really getting together someones looking particular term dont know dont know would bring us much theyre looking say sunny day know sunny shows document day shows theyre together toge dont know doesnt youre looking term relevance kind irrelevant term shows shows right sense single single word well create new well still search still nxt search right doesnt show know itll well wouldnt show search results didnt exist together well well talk pernilla cause see shes cause using mean make sense actually well thats sorta thats true well thing also mean well see mean could actually put list meetings r get returned fifteen meetings language gonna show rank highest see well thing mean thatll exactly looking two words separately see exist document theres possibility c occur together nxt search document well right anything dont know dont know well thou think thats true look well think'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "b52b3dc4",
      "metadata": {
        "id": "b52b3dc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89719ebd-068e-4249-d722-14790c115aef"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': \"nxt is a nfl search engine that uses a search engine to search for a specific topic . the search engine has compiled a list of ten different types of information that can be accessed by a single user . a number of 'seens' are used to find a topic that has been viewed by users . it's a good idea to use a different type of information to search .\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, framework=\"tf\")\n",
        "# summarizer = pipeline('text2text-generation', model, tokenizer=tokenizer, framework=\"tf\")\n",
        "\n",
        "summarizer(doc, min_length=10, max_length=MAX_TARGET_LENGTH)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate Manually\n",
        "\n",
        "Now let's try tokenizing a document from the training set. Don't forget to add 'summarize:' at the start if you're using a `T5` model."
      ],
      "metadata": {
        "id": "JV4oI2RNvhJ1"
      },
      "id": "JV4oI2RNvhJ1"
    },
    {
      "cell_type": "code",
      "source": [
        "doc = raw_datasets[\"val\"][0][\"transcript\"]\n",
        "sum = raw_datasets[\"val\"][0][\"summary\"]\n",
        "\n",
        "if 't5' in MODEL_CHECKPOINT: \n",
        "    document = \"summarize: \" + doc\n",
        "tokenized = tokenizer([doc], return_tensors='np')\n",
        "out = model.generate(**tokenized, max_length=128)"
      ],
      "metadata": {
        "id": "XkN8P7TO2ESN"
      },
      "id": "XkN8P7TO2ESN",
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with tokenizer.as_target_tokenizer():\n",
        "    print(tokenizer.decode(out[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3sZZM5Zs2HNU",
        "outputId": "1c469b4d-2934-4318-9717-03c6454874af"
      },
      "id": "3sZZM5Zs2HNU",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pad><extra_id_0> id camera thats gonna favourite animal snow seemed say spring finally go yes well hundred percent profit twelve fifty find dark often hard know button youre pushing theres remote controls theres kind hidden panel buttons dont really use unless youre programming something thats useful number buttons power button shaped remote interface industrial marketing expert.</s><pad><pad>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sum"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "Vw01B89n2KBP",
        "outputId": "4cbc8f96-dfb8-4485-e3ce-0ec322d5004c"
      },
      "id": "Vw01B89n2KBP",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The project manager opens the meeting by introducing herself and asking everyone to say their name and role in the group. She then states the agenda of the meeting and tells them that they will be designing and creating a new remote control that should be trendy and user-friendly. The meetings will focus on functional, conceptual, and detailed design. Next, each group member draws their favorite animal on the whiteboard and explains the characteristics of that animal. After that the project manager covers the project budget, and then they begin discussing their personal experiences with remote controls and how they want their remote to look. Then the project manager closes the meeting by telling each group member what to do in preparation for the next meeting.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "2c96a916",
      "metadata": {
        "id": "2c96a916"
      },
      "outputs": [],
      "source": [
        "doc = raw_datasets[\"test\"][0]['transcript']\n",
        "\n",
        "if 't5' in MODEL_CHECKPOINT: \n",
        "    document = \"summarize: \" + doc\n",
        "tokenized = tokenizer([doc], return_tensors='np')\n",
        "out = model.generate(**tokenized, max_length=128)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with tokenizer.as_target_tokenizer():\n",
        "    print(tokenizer.decode(out[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnkCNgabxA5N",
        "outputId": "9a4408fc-e53a-42a7-97c0-27a51efa1857"
      },
      "id": "RnkCNgabxA5N",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pad> nxt is a nfl search engine that uses a search engine to search for a specific topic. the search engine has compiled a list of ten different types of information that can be accessed by a single user. a number of'seens' are used to find a topic that has been viewed by users. it's a good idea to use a different type of information to search.</s>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3606: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "TPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c76bc0041a414f6faaad443f96be696b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3093b562cc0d4877b2b05bad1d829008",
              "IPY_MODEL_21514d73f54849caaf01a92b767fe065",
              "IPY_MODEL_3344ea41fb744f5d8aef9311a2e78b99"
            ],
            "layout": "IPY_MODEL_51637d2805d04469bdb99abdac9decaa"
          }
        },
        "3093b562cc0d4877b2b05bad1d829008": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbd11ccb6a604566aca0aa1f9ea7d7e6",
            "placeholder": "​",
            "style": "IPY_MODEL_64be13c57ab84a96bdbc88b44bf9c4d7",
            "value": "Map: 100%"
          }
        },
        "21514d73f54849caaf01a92b767fe065": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0cb435c23f0145a39e08b81bb362029b",
            "max": 120,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f68ef77c4bf848f497abd43de90dca1a",
            "value": 120
          }
        },
        "3344ea41fb744f5d8aef9311a2e78b99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e713475c78d54244bc04786744ad53a4",
            "placeholder": "​",
            "style": "IPY_MODEL_4daabcadd57e4ef9b9009061710be395",
            "value": " 120/120 [00:07&lt;00:00, 15.91 examples/s]"
          }
        },
        "51637d2805d04469bdb99abdac9decaa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "cbd11ccb6a604566aca0aa1f9ea7d7e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64be13c57ab84a96bdbc88b44bf9c4d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0cb435c23f0145a39e08b81bb362029b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f68ef77c4bf848f497abd43de90dca1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e713475c78d54244bc04786744ad53a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4daabcadd57e4ef9b9009061710be395": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "93c83cde52cb4d0ab420dc9c9dcb1d29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_272399d1097e4a16a3c013a636b40019",
              "IPY_MODEL_a6c7050c742d407e8b096c9672135368",
              "IPY_MODEL_ed409ffbdcaa4280acd7e6eac7788930"
            ],
            "layout": "IPY_MODEL_69551e3b032f443d89615435bdf36d8e"
          }
        },
        "272399d1097e4a16a3c013a636b40019": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19ff556d873845a1843ca0981be4b6e3",
            "placeholder": "​",
            "style": "IPY_MODEL_99c6f6a912e64682ba4a8d6f4e6d5db0",
            "value": "Map: 100%"
          }
        },
        "a6c7050c742d407e8b096c9672135368": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_174e7f460f3c4754b893a474bba8cf31",
            "max": 22,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8ec87c74102b4d8790c8477b9699df9c",
            "value": 22
          }
        },
        "ed409ffbdcaa4280acd7e6eac7788930": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94b598e10455409291fc6899aa1cf7d1",
            "placeholder": "​",
            "style": "IPY_MODEL_e38311aaffa8407d97f31ce391043ebe",
            "value": " 22/22 [00:00&lt;00:00, 23.08 examples/s]"
          }
        },
        "69551e3b032f443d89615435bdf36d8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "19ff556d873845a1843ca0981be4b6e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99c6f6a912e64682ba4a8d6f4e6d5db0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "174e7f460f3c4754b893a474bba8cf31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ec87c74102b4d8790c8477b9699df9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "94b598e10455409291fc6899aa1cf7d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e38311aaffa8407d97f31ce391043ebe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "abc65ccc758f4268ac61a1522f6fe66d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c7894d279695404d9b7e286001c0460d",
              "IPY_MODEL_eb090d25112f4b5dba83d266e5527aa7",
              "IPY_MODEL_2e4c45040206446c9e48bc3735aa548c"
            ],
            "layout": "IPY_MODEL_a33afbfd764f492a951529c2de408579"
          }
        },
        "c7894d279695404d9b7e286001c0460d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_614b818bc469417f801f84aaac35801f",
            "placeholder": "​",
            "style": "IPY_MODEL_1454e2f9cef5437a9d60dfb83d5fb520",
            "value": "Map: 100%"
          }
        },
        "eb090d25112f4b5dba83d266e5527aa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c09cd61fdc63402abb20eef3d875314e",
            "max": 29,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_54868df9eb8745078b1834442fd936ac",
            "value": 29
          }
        },
        "2e4c45040206446c9e48bc3735aa548c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3b1492792594f22a45764709f11c951",
            "placeholder": "​",
            "style": "IPY_MODEL_1c5cedd4cd5749259530cc5fc0aac76b",
            "value": " 29/29 [00:02&lt;00:00, 13.26 examples/s]"
          }
        },
        "a33afbfd764f492a951529c2de408579": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "614b818bc469417f801f84aaac35801f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1454e2f9cef5437a9d60dfb83d5fb520": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c09cd61fdc63402abb20eef3d875314e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54868df9eb8745078b1834442fd936ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c3b1492792594f22a45764709f11c951": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c5cedd4cd5749259530cc5fc0aac76b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}