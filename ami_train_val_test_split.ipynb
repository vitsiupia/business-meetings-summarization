{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMoLbugIe2KU+rXhFe2YN+a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vitsiupia/projektPython/blob/main/ami_train_val_test_split.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Je≈õli chcemy, ≈ºeby podczas treningu wy≈õwietla≈Ço siƒô nam \"accuracy\" to musimy mieƒá tak≈ºe dane walidacyjne. "
      ],
      "metadata": {
        "id": "1DyuztbSbh1S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zak≈Çadamy, ≈ºe nasze dane treningowe wyglƒÖdajƒÖ tak: transkrypt-podsumowanie.\n",
        "\n",
        "\n",
        "ü§î... Ale czy rzeczywi≈õcie mamy wszystkie podsumowania odpowiadajƒÖce wszystkim transkryptom? Sprawd≈∫my to!"
      ],
      "metadata": {
        "id": "vNLo1-1Tdgl6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pobieramy dane z github\n",
        "!wget 'https://github.com/vitsiupia/projektPython/raw/main/ami_meetings_preprocessed.zip'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohZS0aSsd2aX",
        "outputId": "b657db5f-4892-4b28-b998-8bc5a148a42b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-14 19:29:34--  https://github.com/vitsiupia/projektPython/raw/main/ami_meetings_preprocessed.zip\n",
            "Resolving github.com (github.com)... 140.82.113.3\n",
            "Connecting to github.com (github.com)|140.82.113.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/vitsiupia/projektPython/main/ami_meetings_preprocessed.zip [following]\n",
            "--2023-05-14 19:29:34--  https://raw.githubusercontent.com/vitsiupia/projektPython/main/ami_meetings_preprocessed.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1038298 (1014K) [application/zip]\n",
            "Saving to: ‚Äòami_meetings_preprocessed.zip‚Äô\n",
            "\n",
            "ami_meetings_prepro 100%[===================>]   1014K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-05-14 19:29:34 (18.7 MB/s) - ‚Äòami_meetings_preprocessed.zip‚Äô saved [1038298/1038298]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Rozpakowujemy je\n",
        "import zipfile\n",
        "# Unzip the file\n",
        "zip_ref = zipfile.ZipFile('ami_meetings_preprocessed.zip')\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "metadata": {
        "id": "roqtitFbfb5T"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "transcripts_dir = 'ami_meetings_preprocessed/transcripts/'\n",
        "abstractive_dir = 'ami_meetings_preprocessed/abstractive/'\n",
        "\n",
        "transcripts_files = set(os.listdir(transcripts_dir))\n",
        "abstractive_files = set(os.listdir(abstractive_dir))\n",
        "\n",
        "transcripts_files_shortened = set([filename.split('.')[0] for filename in transcripts_files])\n",
        "abstractive_files_shortened = set([filename.split('.')[0] for filename in abstractive_files])\n",
        "\n",
        "missing_transcripts = transcripts_files_shortened - abstractive_files_shortened\n",
        "missing_abstractive = abstractive_files_shortened - transcripts_files_shortened\n",
        "\n",
        "if len(missing_transcripts) == 0 and len(missing_abstractive) == 0:\n",
        "    print(\"Dla ka≈ºdego pliku transkryptu istnieje odpowiadajƒÖce mu podsumowanie, i odwrotnie.\")\n",
        "else:\n",
        "    if len(missing_transcripts) > 0:\n",
        "        print(\"Brak podsumowania dla nastƒôpujƒÖcych plik√≥w transkrypt√≥w:\")\n",
        "        for filename in missing_transcripts:\n",
        "            print(filename)\n",
        "\n",
        "    if len(missing_abstractive) > 0:\n",
        "        print(\"Brak transkryptu dla nastƒôpujƒÖcych plik√≥w podsumowa≈Ñ:\")\n",
        "        for filename in missing_abstractive:\n",
        "            print(filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ijm63RihtoS",
        "outputId": "224f2368-1a3a-4f5c-a6f1-378c189bbd37"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Brak podsumowania dla nastƒôpujƒÖcych plik√≥w transkrypt√≥w:\n",
            "IN1009\n",
            "EN2001b\n",
            "EN2002d\n",
            "IN1012\n",
            "IB4001\n",
            "IB4004\n",
            "IN1001\n",
            "EN2006a\n",
            "EN2004a\n",
            "EN2009c\n",
            "EN2001a\n",
            "IB4002\n",
            "EN2009b\n",
            "EN2009d\n",
            "EN2005a\n",
            "EN2002b\n",
            "IN1002\n",
            "IN1008\n",
            "EN2002c\n",
            "IN1005\n",
            "IN1007\n",
            "EN2003a\n",
            "IN1013\n",
            "EN2001d\n",
            "EN2001e\n",
            "IN1014\n",
            "EN2006b\n",
            "IN1016\n",
            "EN2002a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(os.listdir(transcripts_dir))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQcHWB1Rkf_B",
        "outputId": "187bad2e-48ed-4526-e88c-bf5321b74d5e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "171"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(missing_transcripts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1mJ5y07ixtS",
        "outputId": "6d8b90d8-41df-4330-87d9-444904184bf7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "29"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Teraz wszystko jasne! Te zagubione transkrypty pos≈Çu≈ºƒÖ nam p√≥≈∫niej do testowania przetrenowanego modelu."
      ],
      "metadata": {
        "id": "U-fp-IvziA4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tyle danych mamy do podzielenia na treningowe i walidacyjne:\")\n",
        "len(os.listdir(transcripts_dir)) - len(missing_transcripts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6RH1thXnA-7",
        "outputId": "d807a8ce-8b71-4819-e85f-3e9476f71ca9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tyle danych mamy do podzielenia na treningowe i walidacyjne:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "142"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "transcripts_folder = 'ami_meetings_preprocessed/transcripts'\n",
        "summaries_folder = 'ami_meetings_preprocessed/abstractive'\n",
        "test_folder = 'meetings_split/test'\n",
        "train_folder = 'meetings_split/train'\n",
        "val_folder = 'meetings_split/val'\n",
        "\n",
        "# create test folder and move transcripts without summaries\n",
        "os.makedirs(test_folder, exist_ok=True)\n",
        "for file in os.listdir(transcripts_folder):\n",
        "    if not file.endswith('.transcript.txt'):\n",
        "        continue\n",
        "    summary_file = file.replace('.transcript.txt', '.abssumm.txt')\n",
        "    if not os.path.exists(os.path.join(summaries_folder, summary_file)):\n",
        "        shutil.move(os.path.join(transcripts_folder, file), os.path.join(test_folder, file))\n",
        "\n",
        "# create train and val folders and move remaining transcripts\n",
        "os.makedirs(train_folder, exist_ok=True)\n",
        "os.makedirs(val_folder, exist_ok=True)\n",
        "transcript_files = [f for f in os.listdir(transcripts_folder) if f.endswith('.transcript.txt')]\n",
        "train_size = int(0.85 * len(transcript_files))\n",
        "train_files = transcript_files[:train_size]\n",
        "val_files = transcript_files[train_size:]\n",
        "\n",
        "for file in train_files:\n",
        "    shutil.move(os.path.join(transcripts_folder, file), os.path.join(train_folder, file))\n",
        "    summary_file = file.replace('.transcript.txt', '.abssumm.txt')\n",
        "    shutil.move(os.path.join(summaries_folder, summary_file), os.path.join(train_folder, summary_file))\n",
        "\n",
        "for file in val_files:\n",
        "    shutil.move(os.path.join(transcripts_folder, file), os.path.join(val_folder, file))\n",
        "    summary_file = file.replace('.transcript.txt', '.abssumm.txt')\n",
        "    shutil.move(os.path.join(summaries_folder, summary_file), os.path.join(val_folder, summary_file))"
      ],
      "metadata": {
        "id": "aqoJjrPkkuNf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, teraz wyeksportujmy to do zip i pobierzmy."
      ],
      "metadata": {
        "id": "MDzhvXT-vlTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "def zipdir(path, ziph):\n",
        "    # ziph is zipfile handle\n",
        "    for root, dirs, files in os.walk(path):\n",
        "        for file in files:\n",
        "            ziph.write(os.path.join(root, file))\n",
        "zipf = zipfile.ZipFile('meetings_split.zip', 'w', zipfile.ZIP_DEFLATED)\n",
        "zipdir('meetings_split', zipf)\n",
        "zipf.close()"
      ],
      "metadata": {
        "id": "gvCQ64k4vqvN"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}